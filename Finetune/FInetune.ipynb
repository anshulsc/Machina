{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training LLMs with Small Datasets\n",
        "\n",
        "\n",
        "1.   Understanding Model Size\n",
        "2.   Quantization\n",
        "3.   Basic FIne Tuning\n",
        "4.   Advanced Fine Tuning\n",
        "\n"
      ],
      "metadata": {
        "id": "k5OIAwjjuAYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Model Size\n",
        "\n",
        "Llama 2 :\n",
        "- 7b  (no. of weights)\n",
        "- 13b\n",
        "- 70b\n",
        "\n",
        "Each weight represented by 32-bits\n",
        "- 8 bits per byte\n",
        "- 70b model\n",
        "=> 70b x 32 bits/ 8bits per byte = 280 GB approx (Size of weights)\n",
        "\n",
        "\n",
        "Llama 7b:\n",
        "- 7b x 32 / 8 = 28 GB.\n",
        "\n",
        "A100 Nvidia - 40 GB, 80GB\n"
      ],
      "metadata": {
        "id": "rmLoDivuuhPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qunatization\n",
        "\n",
        "Instead of using 32-bits to represent a weight. You sacle it to 4-bits.\n",
        "You lose precision but you can manage it in small gpus\n",
        "\n",
        "2^4 = 32\n",
        "2^32 = ........\n",
        "\n",
        "7b model:\n",
        " -7b x 4 bits / 8 bits per byte= 3.5 GB"
      ],
      "metadata": {
        "id": "CVFNTuI8w86F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning with QLora\n",
        "\n",
        "Quantized LoRa - means training with quantized weights (4-bit in this case)\n",
        "\n",
        "LoRa - Low RanK Adaptation -> freeze pre-trained model weights and injects trainable rank decomposition matrices into each layer of transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "zbdwuv8fxBGl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1lrgds3rj8D",
        "outputId": "e3f98d25-3cf2-4671-cdee-c76bc35820df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model to use: Llama-7B"
      ],
      "metadata": {
        "id": "JdxfYQenzEJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ],
      "metadata": {
        "id": "_tBG3855zI0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Setup"
      ],
      "metadata": {
        "id": "QsmIlcbf1T-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ],
      "metadata": {
        "id": "86lwK5rm1Wbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "  \"\"\"\n",
        "  Prints the number of trainable params\n",
        "  \"\"\"\n",
        "  trainable_params = 0\n",
        "  all_params = 0\n",
        "  for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "\n",
        "  print(f\"trainable params : {trainable_params} || all_params: {all_param} || trainable: {100* {trainable_params/all_param}}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-p8Knx8P2P65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"self_attn.q_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias = \"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "xmbiYH5s3Epb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "qWBn2flU48Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples : tokenizer(samples[\"quote\"]), batched=True)"
      ],
      "metadata": {
        "id": "qZjDoVNW3yjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "U-D8x5lO5P5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# needed for Llama tokenizer\n",
        "tokenizer.pad_token - tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model = model,\n",
        "    train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=10,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Q1Keb4SQ5WEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "K2_eswew7riZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer"
      ],
      "metadata": {
        "id": "DbRAlxt27tOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a stream\n",
        "def stream(user_prompt):\n",
        "  runtimeFlag = \"cuda:0\"\n",
        "  sys_prompt = 'You are helpful assistant that blah blah'\n",
        "  B_INST, E_INST = \"[INST]\", \"[\\INST]\"\n",
        "  B_SYS, E_SYS = \"[SYS]\", \"[\\SYS]\"\n",
        "\n",
        "  prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
        "\n",
        "  inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
        "\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "\n",
        "  _ = model.generate(**inputs,streamer=streamer, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "24MHvH4d70MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Fine Tuning\n",
        "- Prompt masking\n",
        "- End of sequence token\n",
        "\n",
        "Attention:\n",
        "- is the idea that the prediction of the next token depends on earlier tokens\n",
        "\n",
        "[The][ quick][ brown][ fox][ jumped][...]\n",
        "[1]     [1]     [1]    [1]    [1]\n",
        "\n",
        "|<pad>| [The][ quick][ brown][ fox][ jumped][...]\n",
        "[0]     [1]     [1]    [1]    [1]\n",
        "\n",
        "Loss Masking:\n",
        "- selecting what token predictions to penalize\n",
        "\n",
        "Inputs: [The][ quick][ brown][ fox][ jumped]\n",
        "Predic: [boy][brown][fox][jumped][ over]\n",
        "Actual: [ quick][ brown][ fox][ jumped][over]\n",
        "Losses:  [5]    [0.3]    [0.02]  [0.1] [0.3]"
      ],
      "metadata": {
        "id": "-7ZdA-c0_jsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link- for training LLama for function calls](https://www.youtube.com/watch?v=OQdp-OeG1as)"
      ],
      "metadata": {
        "id": "Yc5gz5uA7zTi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0hZnaS4QH-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}